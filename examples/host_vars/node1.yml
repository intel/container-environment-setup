---
# Kubernetes node configuration

sriov_enabled: false
# sriov_nics: PF interfaces names
sriov_nics: []
sriov_numvfs: 0
sriov_cni_enabled: true

# If sriov_net_dp_enabled=true in all.yml adjust and uncomment below configuration:
#sriov_net_dp_config:
#- pfnames: ["enp175s0f0"]     # PF interface names - their VFs will be attached to specific driver
#  driver: "i40evf"            # available options:  "i40evf", "vfio-pci", "igb_uio"
#- pfnames: ["enp175s0f1"]     # PF interface names - their VFs will be attached to specific driver
#  driver: "vfio-pci"          # available options:  "i40evf", "vfio-pci", "igb_uio"
#- pfnames: ["enp175s0f2"]     # PF interface names - their VFs will be attached to specific driver
#  driver: "igb_uio"           # available options:  "i40evf", "vfio-pci", "igb_uio"

userspace_cni_enabled: true
vpp_enabled: true
ovs_dpdk_enabled: true
# CPU mask for OVS-DPDK PMD threads
ovs_dpdk_lcore_mask: 0x1
# Huge memory pages allocated by OVS-DPDK per NUMA node in megabytes
# example 1: "256,512" will allocate 256MB from node 0 abd 512MB from node 1
# example 2: "1024" will allocate 1GB fron node 0 on a single socket board, e.g. in a VM
ovs_dpdk_socket_mem: "256,0"

# Set to 'true' to update i40e and i40evf kernel modules
force_nic_drivers_update: true

# Enables hugepages support
hugepages_enabled: true

# Hugepage sizes available: 2M, 1G
default_hugepage_size: 2M

# Sets how many hugepages of each size should be created
hugepages_1G: 0
hugepages_2M: 128

# CPU isolation from Linux scheduler
isolcpus_enabled: true
isolcpus: "4-7"
